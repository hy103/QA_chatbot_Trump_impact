{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e9053-c9a8-44ce-b82b-1505d98a1484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a3083-a632-475b-88b2-e800f9af708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5584fc7-b89e-4b20-b0c2-7b858d1cf73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815b4113-846e-4a73-98eb-cf724e49e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model = \"gpt-4o-mini\", api_key = \"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d556506-f3a9-42f1-9c12-c75827bd9ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a0c6da-064b-4ac5-8c57-269d24806138",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install weaviate-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace8e8e-51d6-4bbd-b34f-43724de92108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = [model.encode(doc.page_content) for doc in splits]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466fa895-c480-4a0c-8b5f-ebdf7f429432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths = (\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs = dict(\n",
    "        parse_only = bs4.SoupStrainer(\n",
    "            class_ = (\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae96168-9aae-4d67-b45c-e035bc139913",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents = splits, embedding = OpenAIEmbeddings())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9930c34d-348b-4489-bf78-55ae626cbd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320577b-0813-4838-8047-ed10a0e6ac69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f68fd24-eba3-4ebf-a1f4-5703cb025a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50209fa-1071-4315-bcdb-01038439bf8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b396c2db-39c7-4250-abfb-18b7f66f5f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83412081-1b17-4007-b878-6e4278be6499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8fe2608-e741-4993-90fe-093e6d8fe7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from faiss import IndexFlatL2\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Step 1: Fetch HTML Content\n",
    "url = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    html_content = response.text\n",
    "else:\n",
    "    raise Exception(f\"Failed to fetch webpage. Status code: {response.status_code}\")\n",
    "\n",
    "# Step 2: Parse HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Extract specific content (e.g., from the main post body)\n",
    "content = soup.find(class_=\"post-content\")  # Adjust the class to match the blog structure\n",
    "if content:\n",
    "    text = content.get_text()\n",
    "else:\n",
    "    raise Exception(\"Failed to find content with the specified class.\")\n",
    "\n",
    "# Step 3: Split Text into Chunks\n",
    "chunk_size = 1000\n",
    "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79ff14c-aa0d-42c3-ae49-e3c5d3483e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate Embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = [model.encode(chunk) for chunk in chunks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ea0403-f816-41c8-b221-0690c40de0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Vector Store\n",
    "index = IndexFlatL2(len(embeddings[0]))\n",
    "index.add(np.array(embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df028c03-cca3-4e16-96bd-6bc187c93649",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be149cb1-7179-4ed9-bb34-7f3759d32cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def retrieve(query, k=5):\n",
    "    query_embedding = model.encode(query)\n",
    "    distances, indices = index.search(np.array([query_embedding]), k)\n",
    "    return [chunks[i] for i in indices[0]]\n",
    "\n",
    "def generate_response(context, question):\n",
    "    input_text = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = llm_model.generate(inputs['input_ids'], max_length=200)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d535f4-2042-4c09-950e-5f5760c7984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Query the RAG system\n",
    "query = \"What is Task Decomposition?\"\n",
    "retrieved_docs = retrieve(query)\n",
    "context = \"\\n\".join(retrieved_docs)\n",
    "response = generate_response(context, query)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9177bcf3-a51d-4e5c-a94a-142a804487ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and processing webpage content...\n",
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyarrava/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving relevant context...\n",
      "Generating answer...\n",
      "\n",
      "Generated Answer:\n",
      "Context: t needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) b  evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups  ws the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\n",
      "\n",
      "Fig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
      "The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\n",
      "Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying \n",
      "\n",
      "Question: What is task decomposition?\n",
      "\n",
      "Answer: Task decomposition is a process of decomposing a problem into smaller and simpler steps. The decomposition process is performed by a model, which is a set of rules that describe how the model can be used to solve the problem. The rules are used to generate a plan for the problem. The plan is then used to generate a sequence of actions that can be executed to solve the problem. The plan is then used to generate a sequence of actions that can be executed to solve the problem. The plan is then used to generate a sequence of actions that can be executed to solve the problem. The plan is then used to generate a sequence of actions that can be executed to solve the problem. The plan is then used to generate a sequence of actions that can be executed to solve the problem. The plan is then used to generate a sequence of actions that can be executed to solve the problem. The plan is then used to generate a sequence of actions that can be executed to solve the problem. The plan is then used to generate a sequence of actions that can be executed to solve the problem. The plan is then used to generate a sequence of actions that can be executed to solve the problem. The plan is then used to generate a sequence of actions that can be executed to solve the problem. The plan is then used to generate a sequence of actions that can be executed to solve the problem. The plan is then used to generate a sequence of actions that can be executed to solve the problem. The plan is then used to generate a sequence of actions that can be executed to solve the problem. The plan is then used to generate a sequence of actions that can be executed to solve the problem. The plan is then used to generate a sequence of actions that\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Step 1: Scrape the webpage content\n",
    "def fetch_webpage_content(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        content = soup.find(class_=\"post-content\")  # Adjust class to match webpage\n",
    "        if content:\n",
    "            return content.get_text()\n",
    "        else:\n",
    "            raise Exception(\"Content not found on the page!\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to fetch webpage. Status code: {response.status_code}\")\n",
    "\n",
    "# Step 2: Split text into chunks\n",
    "def split_text_into_chunks(text, chunk_size=1000, chunk_overlap=200):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size - chunk_overlap):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "# Step 3: Embed chunks and build FAISS index\n",
    "def build_faiss_index(chunks, embedder_model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    embedder = SentenceTransformer(embedder_model_name)\n",
    "    embeddings = embedder.encode(chunks)\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    # Create FAISS index\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(np.array(embeddings))\n",
    "    return index, embedder\n",
    "\n",
    "# Step 4: Retrieve relevant chunks\n",
    "def retrieve_relevant_chunks(query, index, chunks, embedder):\n",
    "    query_embedding = embedder.encode([query])\n",
    "    distances, indices = index.search(np.array(query_embedding), k=3)\n",
    "    relevant_chunks = [chunks[idx] for idx in indices[0]]\n",
    "    return \" \".join(relevant_chunks)\n",
    "\n",
    "# Step 5: Generate answers using Bloom\n",
    "def generate_answer(context, question, model_name=\"bigscience/bloom-560m\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    outputs = model.generate(**inputs, max_length=1024, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Step 6: Build the RAG pipeline\n",
    "def rag_pipeline(url, question):\n",
    "    print(\"Fetching and processing webpage content...\")\n",
    "    content = fetch_webpage_content(url)\n",
    "    chunks = split_text_into_chunks(content)\n",
    "    \n",
    "    print(\"Building FAISS index...\")\n",
    "    index, embedder = build_faiss_index(chunks)\n",
    "    \n",
    "    print(\"Retrieving relevant context...\")\n",
    "    context = retrieve_relevant_chunks(question, index, chunks, embedder)\n",
    "    \n",
    "    print(\"Generating answer...\")\n",
    "    answer = generate_answer(context, question)\n",
    "    return answer\n",
    "\n",
    "# Step 7: Run the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"  # Example blog URL\n",
    "    question = \"What is task decomposition?\"\n",
    "    \n",
    "    answer = rag_pipeline(url, question)\n",
    "    print(\"\\nGenerated Answer:\")\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2135ef1-5b30-4abe-8e7d-0922660bf8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c018c6-887e-4cac-bce2-9a9d966b0899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae5be9-6d2d-4bfe-a00e-0b8432a1011f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca9463-f6fd-45d8-890f-39f96299a4b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "170dcab6-4699-4853-9be9-7a88b9f724fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Step 1: Extract text from a PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Step 2: Split text into chunks\n",
    "def split_text_into_chunks(text, chunk_size=1000, chunk_overlap=200):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size - chunk_overlap):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "# Step 3: Embed chunks and build FAISS index\n",
    "def build_faiss_index(chunks, embedder_model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    embedder = SentenceTransformer(embedder_model_name)\n",
    "    embeddings = embedder.encode(chunks)\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    # Create FAISS index\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(np.array(embeddings))\n",
    "    return index, embedder\n",
    "\n",
    "# Step 4: Retrieve relevant chunks\n",
    "def retrieve_relevant_chunks(query, index, chunks, embedder):\n",
    "    query_embedding = embedder.encode([query])\n",
    "    distances, indices = index.search(np.array(query_embedding), k=3)\n",
    "    relevant_chunks = [chunks[idx] for idx in indices[0]]\n",
    "    return \" \".join(relevant_chunks)\n",
    "\n",
    "# Step 5: Generate answers using Bloom\n",
    "def generate_answer(context, question, model_name=\"bigscience/bloom-560m\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    outputs = model.generate(**inputs, max_length=1024, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Step 6: Build the RAG pipeline for PDF\n",
    "def rag_pipeline_from_pdf(pdf_path, question):\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    content = extract_text_from_pdf(pdf_path)\n",
    "    chunks = split_text_into_chunks(content)\n",
    "    \n",
    "    print(\"Building FAISS index...\")\n",
    "    index, embedder = build_faiss_index(chunks)\n",
    "    \n",
    "    print(\"Retrieving relevant context...\")\n",
    "    context = retrieve_relevant_chunks(question, index, chunks, embedder)\n",
    "    \n",
    "    print(\"Generating answer...\")\n",
    "    answer = generate_answer(context, question)\n",
    "    return answer\n",
    "\n",
    "# Step 7: Run the chatbot for PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1306968a-347e-4ed2-bf34-40a92a1475b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rag_pipeline_from_pdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Data/PC_US_Elections_clean.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Path to your PDF file\u001b[39;00m\n\u001b[1;32m      3\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are effects on India if Donald trump wins?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mrag_pipeline_from_pdf\u001b[49m(pdf_path, question)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerated Answer:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rag_pipeline_from_pdf' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"../Data/PC_US_Elections_clean.pdf\"  # Path to your PDF file\n",
    "    question = \"What are effects on India if Donald trump wins?\"\n",
    "    \n",
    "    answer = rag_pipeline_from_pdf(pdf_path, question)\n",
    "    print(\"\\nGenerated Answer:\")\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3469a35-3e89-449d-a06a-ae83883df9f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
